# -*- coding: utf-8 -*-
"""bikesharing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VEd1CJw5bQrXXt_6hTkoNhD4x70Nm547
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as mp
import seaborn as sns
import statsmodels
import statsmodels.api as sm
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import MinMaxScaler

import warnings
warnings.filterwarnings

from google.colab import drive
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/

"""**Reading the DataSet**"""

bike=pd.read_csv('day.csv')
bike.head()

bike.info()

"""**Exploratory Data Analysis**"""

bike=bike.drop(['instant','dteday','casual','registered','atemp'],axis=1)

"""1. **instant** and **dteday** are dropped because instabce has no significance as its row number. dteday has columns like year and month which tell he same story
2. **casual**,**registered**,**atemp** are dropped because they have multicolinear with **cnt** and **temp** resepectively
"""

bike.shape

bike[['weekday','mnth','weathersit','season']]=bike[['weekday','mnth','weathersit','season']].astype(str,copy=True)

"""converted the datatype of catagorical columns to String Type"""

bike['season']=bike['season'].map({'1':'spring', '2':'summer', '3':'fall', '4':'winter'})

bike['weekday']=bike['weekday'].map({'0':'sunday','1':'monday','2':'tuesday','3':'wednesday','4':'thursday','5':'friday','6':'saturday'})

bike['weathersit']=bike['weathersit'].map({'1':'clear','2':'mist_cloudy','3':'light_rain'})

bike.info()

"""**Checking Null Values**"""

bike.isnull().sum()*100/bike.shape[0]

"""No null values present"""

bike.describe()

"""1. No Outliers are present

**Numerical Columns Analysis**
"""

sns.pairplot(bike)
mp.show()

"""1.Temerature is also having linear relationship between cnt

**Categorical Column Analysis**
"""

mp.figure(figsize=(20,12))
mp.subplot(2,4,1)
sns.boxplot(x='season',y='cnt',data=bike)
mp.subplot(2,4,2)
sns.boxplot(x='yr',y='cnt',data=bike)
mp.subplot(2,4,3)
sns.boxplot(x='holiday',y='cnt',data=bike)
mp.subplot(2,4,4)
sns.boxplot(x='weekday',y='cnt',data=bike)
mp.subplot(2,4,5)
sns.boxplot(x='workingday',y='cnt',data=bike)
mp.subplot(2,4,6)
sns.boxplot(x='weathersit',y='cnt',data=bike)
mp.subplot(2,4,7)
sns.boxplot(x='mnth',y='cnt',data=bike)
mp.show()

"""1. Fall and Summer has high mean so thease seasons have high bike rents
2. Year 2019 has high bike rents comapred to 2018
3. Friday has high meann and distribution follwed by saturday
4. Month Aug and Sep has high mean valuse so, thease months has high bike rents compared to other months
5. Weather is Clear bike rents are high

**Data Preparation**

**Dummyfication**
"""

status=pd.get_dummies(bike[['weekday','mnth','weathersit','season']],drop_first=True)
status.head()

"""1. weekday,mnth,weathersit,season are dummyfied
2. dropped first variable as we know for **P** variables **P-1** variables are sufficient 
"""

bike=pd.concat([bike,status],axis=1)
bike.info()

bike.rename(columns={'weathersit_1':'Clear',	'weathersit_2':'Mist_Cloudy',	'weathersit_3':'Light_Snow_Rain ','season_1':'spring',	'season_2':'summer',	'season_3':'fall',	'season_4':'winter'},inplace=True)

bike.drop(['season','mnth','weathersit','weekday'],inplace=True,axis=1)

"""**Splitting data in Train and Test dataset**"""

df_train,df_test=train_test_split(bike,train_size=0.7,random_state=100)
df_train.shape

num_var = ['temp','hum','windspeed','cnt']

"""**Scaling : MinMaxScaling**"""

scaler = MinMaxScaler()
df_train[num_var] = scaler.fit_transform(df_train[num_var])
df_train.head()

bike.head()

"""**Finding Corelation**"""

mp.figure(figsize=(30,12))
sns.heatmap(df_train.corr(),annot=True)
mp.show()

"""1. Temp and Fall has +ve corelation(0.7)
2. Temp and Cnt has +ve corelation(0.65)
3. mist_cloudy and low_rain has -ve corelation

**Splitting Train Dataset into x and y data sets**
"""

y_train = df_train.pop('cnt')
x_train = df_train

"""**Model-1 :RFE Method**"""

from sklearn.feature_selection import RFE
lr=LinearRegression()
rfe= RFE(lr, n_features_to_select=16, step=1)
rfe.fit(x_train,y_train)

col = list(x_train.columns[rfe.support_])
col

x_train_rfe = x_train[col]
x_train_rfe.head()

"""**Statistical Inference**"""

x_train_sm=sm.add_constant(x_train_rfe)
lr=sm.OLS(y_train,x_train_sm)
lr_mode=lr.fit()
lr_mode.summary()

"""1. |P| value of month-6 is 0.268 so, we may have to drop that variable 
2. R2 value is 0.845
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif=pd.DataFrame()
vif['Features']=x_train_sm.columns
vif['VIF']=[variance_inflation_factor(x_train_sm.values,i) for i in range(x_train_sm.shape[1])]
vif

"""1. VIF of Spring season  is 6.319
2. VIF should have < 5 so , we may have to drop few variables

**Model-2:**
"""

lr=LinearRegression()
rfe=RFE(lr,n_features_to_select=15,step=1)
rfe.fit(x_train,y_train)

"""reduced number of independent columns to 15"""

col = list(x_train.columns[rfe.support_])
col

x_train_rfe = x_train[col]
x_train_rfe.head()

"""**Inferential Statistics of Model-2**"""

x_train_sm=sm.add_constant(x_train_rfe)
lr=sm.OLS(y_train,x_train_sm)
lr_mode=lr.fit()
lr_mode.summary()

"""1. R2 value is still remain same 0.845
2. Month -2 has high |P|value 
3. we may have to drop the variables
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif=pd.DataFrame()
vif['Features']=x_train_sm.columns
vif['VIF']=[variance_inflation_factor(x_train_sm.values,i) for i in range(x_train_sm.shape[1])]
vif

"""1. Still Spring season has VIF 6.16 so, we may have to drop the variables

**Model-3**

Taking 14 independent variables
"""

lr=LinearRegression()
rfe=RFE(lr,n_features_to_select=14,step=1)
rfe.fit(x_train,y_train)

col=list(x_train.columns[rfe.support_])
col

x_train_rfe=x_train[col]

"""**Inferential Statistics**"""

x_train_sm=sm.add_constant(x_train_rfe)
lr=sm.OLS(y_train,x_train_sm)
lr_model=lr.fit()
lr_model.summary()

"""1. Still Spring season has |P| value 0.112 so, We may have drop few columns"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif=pd.DataFrame()
vif['Features']=x_train_sm.columns
vif['VIF']=[variance_inflation_factor(x_train_sm.values,i) for i in range(x_train_sm.shape[1])]
vif

"""1. VIF of Spring Season is 5.97, so we need to drop few columns

**Model-5**


Taking 13 independent variables
"""

lr=LinearRegression()
rfe=RFE(lr,n_features_to_select=13,step=1)
rfe.fit(x_train,y_train)

col=list(x_train.columns[rfe.support_])
col

x_train_rfe=x_train[col]

x_train_sm=sm.add_constant(x_train_rfe)
lr=sm.OLS(y_train,x_train_sm)
lr_model=lr.fit()
lr_model.summary()

"""1. R2 value is 0.843 but Still |P| value of Spring  is 0.221, we need to drop few more varibles"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif=pd.DataFrame()
vif['Features']=x_train_sm.columns
vif['VIF']=[variance_inflation_factor(x_train_sm.values,i) for i in range(x_train_sm.shape[1])]
vif

"""**Model-6 :: Final Model**


Taking 12 independent variables
"""

lr=LinearRegression()
rfe=RFE(lr,n_features_to_select=12,step=1)
rfe.fit(x_train,y_train)

col=list(x_train.columns[rfe.support_])
col

x_train_rfe=x_train[col]

x_train_sm=sm.add_constant(x_train_rfe)
lr=sm.OLS(y_train,x_train_sm)
lr_model=lr.fit()
lr_model.summary()

"""1. R2 value is 0.842 and None of the |P| value is >0.05. So,This Model will be the final model
2. Temperature,Summer,Winter and month 9 & 8 are +ve significant factors.
3. Holiday and Rainy are having -Ve Significance.
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif=pd.DataFrame()
vif['Features']=x_train_sm.columns
vif['VIF']=[variance_inflation_factor(x_train_sm.values,i) for i in range(x_train_sm.shape[1])]
vif

"""1. None of the VIF value is >5 so, we can say this our final model

**Model Evaluation**

Reading Test data set
"""

df_test.head()

"""**Scaling: MinMaxScaling**"""

df_test[num_var] = scaler.fit_transform(df_test[num_var])
df_test.head()

"""**Splitting Test Dataset into x_teat and y_test**"""

y_test = df_test.pop('cnt')
x_test = df_test

col

x_test_rfe=x_test[col]

x_test_rfe1 = sm.add_constant(x_test_rfe)

x_test_rfe1.info()

"""**Y_Value Prediction**"""

y_pred = lr_model.predict(x_test_rfe1)

"""**R2 Value calculation**"""

from sklearn.metrics import r2_score

r2_score(y_test,y_pred)

"""R2 value of the test data is 0.801

**Proving the Assumptions of LinearRegression**
"""

sns.scatterplot(y_test,y_pred)

"""1. One of the assumptions that is y value and y predicted value is linearly distributed."""

error = y_test-y_pred
sns.distplot(error)

"""1. The Error is Normally distributed"""